{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import statistical and graphical libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pysal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0ec385f9f03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpysal\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pysal'"
     ]
    }
   ],
   "source": [
    "import pysal as ps\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import prettyplotlib as ppl\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import watermark\n",
    "\n",
    "\n",
    "pd.set_option(\"max_columns\", None)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%watermark -v -d -a 'Mario Javier Carrillo' -p matplotlib,numpy,pandas\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -v -d -a 'Mario Javier Carrillo' -p matplotlib,numpy,pandas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/java/Desktop/HORIZON2020/DATASETS/NATURE_CRIME/Type_crime.csv')\n",
    "df.head\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df.rename(columns={'STATE/UT':'STATE_UT','Place Of Occurrence':'PLACE_OCCURENCE',\n",
    "'Dacoity (Section 395-398 IPC) - Number of cases registered':'DACOITY_NUM',\n",
    "    'Dacoity (Section 395-398 IPC) - Value Of Property Stolen (in rupees)':'VALUE_DACOITY',\n",
    "        'Robbery(Section 392-394, 397, 398 IPC) - Number of cases registered':'ROBBERY_NO',\n",
    "            'Robbery(Section 392-394, 397, 398 IPC) - Value Of Property Stolen (in rupees)':'ROBBERY_VALUE',\n",
    "                'Burglary(Section 449-452, 454, 455, 457-460 IPC) - Number of cases registered':'BURGLARY_NO',\n",
    "                    'Burglary(Section 449-452, 454, 455, 457-460 IPC) - Value Of Property Stolen (in rupees)':'BURGLARY_VALUE',\n",
    "                        'Theft (Section 379-382 IPC) - Number of cases registered':'THEFT_NO',\n",
    "                            'Theft (Section 379-382 IPC) - Value Of Property Stolen (in rupees)':'THEFT_VALUE'},inplace=True)\n",
    "#print(df)\n",
    "\n",
    "df.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['STATE_UT'] = df.STATE_UT.str.replace('\\s+&\\s+', '&')\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from altair import *\n",
    "from altair import Chart, Color, Scale, X, Y\n",
    "from altair import Chart, Color, Scale\n",
    "from IPython.display import display\n",
    "\n",
    "def sub_frame(dataset, group_cols=['STATE_UT','YEAR'], col_name = 'PLACE_OCCURENCE', category=\"HIGH-WAY\"):\n",
    "#def sub_frame(dataset, group_cols=['STATE_UT', 'YEAR'], col_name = 'PLACE_OCCURENCE', PLACE_OCCURENCE=\"RESIDENTIAL PREMISES\"):\n",
    "    '''Subsets the original dataframe to a category'''\n",
    "    subset_frame =  dataset[dataset[col_name] == category]\n",
    "    #print(type(subset_frame))\n",
    "    #print(subset_frame.head)\n",
    "    '''Group by group_cols and category, returns df with number of occurrences'''\n",
    "    #group = subset_frame.groupby(group_cols)[col_name].count().reset_index()\n",
    "    group = subset_frame[['STATE_UT','YEAR','DACOITY_NUM']]\n",
    "    chart = alt.Chart(group)\n",
    "    chart.configure_cell(height=400, width=600)\n",
    "    graph = chart.mark_line().encode(\n",
    "        X('YEAR:T', axis=Axis(title='States')),\n",
    "        Y('DACOITY_NUM:Q', axis=Axis(title='Number of crimes on high way')),\n",
    "        color='STATE_UT',)\n",
    "    display(graph)\n",
    "    name =  'Trends_'+category\n",
    "    path = '/home/dbda/Desktop/'\n",
    "    chart.savechart(path+name+'.html')\n",
    "    #return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sub_frame(df, group_cols=['STATE_UT'],  col_name = 'PLACE_OCCURENCE', category='HIGH-WAY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Total number of Dacoity State wise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sub_frame_total(dataset, group_cols=['STATE_UT','YEAR'], col_name = 'PLACE_OCCURENCE', category=\"TOTAL (MANUAL)\"):\n",
    "\n",
    "    '''Subsets the original dataframe to a category'''\n",
    "    subset_frame =  dataset[dataset[col_name] == category]\n",
    "    #print(type(subset_frame))\n",
    "    #print(subset_frame.head)\n",
    "    '''Group by group_cols and category, returns df with number of occurrences'''\n",
    "    #group = subset_frame.groupby(group_cols)[col_name].count().reset_index()\n",
    "    group = subset_frame[['STATE_UT','YEAR','DACOITY_NUM']]\n",
    "    #print(group)\n",
    "    chart = alt.Chart(group)\n",
    "    chart.configure_cell(height=400, width=600)\n",
    "    graph = chart.mark_line().encode(\n",
    "        X('YEAR:T', axis=Axis(title='States')),\n",
    "        Y('DACOITY_NUM:Q', axis=Axis(title='Number of total Dacoity')),\n",
    "        color='STATE_UT',)\n",
    "    display(graph)\n",
    "    name =  'Trends_'+category\n",
    "    path = '/home/dbda/Desktop/'\n",
    "    chart.savechart(path+name+'.html')\n",
    "    #return graph\n",
    "    \n",
    "    print(type(group['DACOITY_NUM']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_frame_total(df, group_cols=['STATE_UT'],  col_name = 'PLACE_OCCURENCE', category='TOTAL (MANUAL)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Number of Robbery state-location wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_frame_group(dataset, group_cols=['STATE_UT','YEAR'], col_name = 'PLACE_OCCURENCE'):\n",
    "#def sub_frame(dataset, group_cols=['STATE_UT', 'YEAR'], col_name = 'PLACE_OCCURENCE', PLACE_OCCURENCE=\"RESIDENTIAL PREMISES\"):\n",
    "    '''Subsets the original dataframe to a category'''\n",
    "    \n",
    "    df_no_total = dataset.loc[dataset.PLACE_OCCURENCE !='TOTAL (MANUAL)', :]\n",
    "    df_no_total_F = df_no_total.loc[df_no_total.PLACE_OCCURENCE !='TOTAL COMPUTER FROM 1 TO 7', :]\n",
    "    \n",
    "        \n",
    "    subset_frame =  df_no_total_F[['STATE_UT','PLACE_OCCURENCE','ROBBERY_NO']]\n",
    "    #print(type(subset_frame))\n",
    "    #print(subset_frame.describe)\n",
    "    '''Group by group_cols and category, returns df with number of occurrences'''\n",
    "    group = subset_frame.groupby(['STATE_UT',col_name]).sum().reset_index()\n",
    "    print(group)\n",
    "    \n",
    "    print(list(group))\n",
    "    chart = alt.Chart(group)\n",
    "    chart.configure_cell(height=400, width=600)\n",
    "    graph = chart.mark_line().encode(\n",
    "        X('STATE_UT', axis=Axis(title='states')),\n",
    "        Y('ROBBERY_NO', axis=Axis(title='Total robbery at different places')),\n",
    "        color='PLACE_OCCURENCE',)\n",
    "    display(graph)\n",
    "    name =  'Trends_ROBBERY_STATS_locationwise'\n",
    "    path = '/home/dbda/Desktop/'\n",
    "    chart.savechart(path+name+'.html')\n",
    "    return graph\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_frame_group(df, group_cols=['STATE_UT'], col_name = 'PLACE_OCCURENCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_frame_group(dataset, group_cols=['STATE_UT','YEAR'], col_name = 'PLACE_OCCURENCE'):\n",
    "#def sub_frame(dataset, group_cols=['STATE_UT', 'YEAR'], col_name = 'PLACE_OCCURENCE', PLACE_OCCURENCE=\"RESIDENTIAL PREMISES\"):\n",
    "    '''Subsets the original dataframe to a category'''\n",
    "    \n",
    "    df_no_total = dataset.loc[dataset.PLACE_OCCURENCE !='TOTAL (MANUAL)', :]\n",
    "    df_no_total_F = df_no_total.loc[df_no_total.PLACE_OCCURENCE !='TOTAL COMPUTER FROM 1 TO 7', :]\n",
    "    \n",
    "        \n",
    "    subset_frame =  df_no_total_F[['STATE_UT','PLACE_OCCURENCE','ROBBERY_NO']]\n",
    "    #print(type(subset_frame))\n",
    "    #print(subset_frame.describe)\n",
    "    '''Group by group_cols and category, returns df with number of occurrences'''\n",
    "    group = subset_frame.groupby(['STATE_UT',col_name]).sum().reset_index()\n",
    "    #print(group)\n",
    "    \n",
    "\n",
    "    chart2 = alt.Chart(group)\n",
    "    chart2.configure_cell(height=400, width=600)\n",
    "    graph1 = chart2.mark_line().encode(\n",
    "        X('PLACE_OCCURENCE', axis=Axis(title='Places')),\n",
    "        Y('ROBBERY_NO', axis=Axis(title='Total robbery at different places')),\n",
    "        color='STATE_UT',)\n",
    "    display(graph1)\n",
    "    name1 =  'Trends_ROBBERY_STATS_statewise'\n",
    "    path1 = '/home/dbda/Desktop/'\n",
    "    chart2.savechart(path1+name1+'.html')\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_frame_group(df, group_cols=['STATE_UT'], col_name = 'PLACE_OCCURENCE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area chart for total crimes vs State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_frame_areatotal(dataset,crime, group_cols=['STATE_UT','YEAR'], col_name = 'PLACE_OCCURENCE', category=\"TOTAL (MANUAL)\"):\n",
    "\n",
    "    '''Subsets the original dataframe to a category'''\n",
    "    subset_frame =  dataset[dataset[col_name] == category]\n",
    "    #print(type(subset_frame))\n",
    "    #print(subset_frame.head)\n",
    "    '''Group by group_cols and category, returns df with number of occurrences'''\n",
    "    #group = subset_frame.groupby(group_cols)[col_name].count().reset_index()\n",
    "    #group_dacoity = subset_frame[['STATE_UT','YEAR','DACOITY_NUM','ROBBERY_NO','THEFT_NO','BURGLARY_NO']]\n",
    "    \n",
    "    group = subset_frame[['STATE_UT','YEAR',crime]]\n",
    "    \n",
    "    #group_robbery = subset_frame[['STATE_UT','YEAR','ROBBERY_NO']]\n",
    "    #group_burglary = subset_frame[['STATE_UT','YEAR','BURGLARY_NO']]\n",
    "    #group_theft = subset_frame[['STATE_UT','YEAR','THEFT_NO']]\n",
    "        \n",
    "    #group = group.groupby(['STATE_UT']).sum().reset_index()\n",
    "    \n",
    "    print(group.head)\n",
    "    \n",
    "    chart = alt.Chart(group)\n",
    "    chart.configure_cell(height=400, width=1000)\n",
    "    graph = chart.mark_area().encode(X('YEAR:T', axis=Axis(axisWidth=0.5,\n",
    "            #format='%M',\n",
    "            labelAngle=0.0,title='Places')),\n",
    "    Y(crime, axis=Axis(title='Total '+crime+'at different places')),color='STATE_UT')\n",
    "    display(graph)\n",
    "    name =  'Trends_'\" in \"+crime+\" \"\n",
    "    path = '/home/dbda/Desktop/'\n",
    "    chart.savechart(path+name+'.html')     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_frame_total(df, group_cols=['STATE_UT'],  col_name = 'PLACE_OCCURENCE', category='TOTAL (MANUAL)')\n",
    "\n",
    "main_crimes = ['DACOITY_NUM', 'ROBBERY_NO', 'BURGLARY_NO','THEFT_NO']\n",
    "for crime in main_crimes:\n",
    "        sub_frame_areatotal(df,crime,group_cols=['STATE_UT','YEAR'],col_name='PLACE_OCCURENCE',  \n",
    "                            category='TOTAL (MANUAL)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_df_trends(dataset, group_by_col=['YEAR'], category='TOTAL (MANUAL)', col_name='number_incidents'):\n",
    "    '''Returns subsets df'''\n",
    "    font_title = {'family': 'serif', 'color':  'black', 'weight': 'normal','size': 8,}\n",
    "    font_axis = {'family': 'serif', 'color':  'black', 'weight': 'normal','size': 10,}\n",
    "    \n",
    "    \n",
    "    subset_frame =  dataset[['YEAR','PLACE_OCCURENCE','DACOITY_NUM', 'ROBBERY_NO', 'BURGLARY_NO','THEFT_NO']]\n",
    "    df_total_num=subset_frame[subset_frame['PLACE_OCCURENCE']==category].groupby(group_by_col)[['DACOITY_NUM', 'ROBBERY_NO', 'BURGLARY_NO','THEFT_NO']].agg('sum').reset_index()\n",
    "    #print(df_none.head)\n",
    "    \n",
    "    df_total_num['number_incidents']= df_total_num.iloc[:, 1:5].sum(axis=1)\n",
    "    \n",
    "    #print(df_total_num.head)\n",
    "    \n",
    "    main_crimes = ['DACOITY_NUM', 'ROBBERY_NO', 'BURGLARY_NO','THEFT_NO']\n",
    "    \n",
    "    for crime in main_crimes:\n",
    "        \n",
    "        df = df_total_num[['YEAR',crime]]\n",
    "        fig = plt.figure(figsize=(13,9))\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(df.YEAR, df[crime], '--', linewidth=2, label='Trend of '+crime+\"\"' Incidents',color=\"y\")\n",
    "        text = \"\\n \" + category.title()+ \"\"+crime+\"\"'  Incidents'\n",
    "        plt.title(text, fontdict=font_title)\n",
    "        plt.ylabel('Total Number of' +crime+\"\" 'Incidents\\n', fontdict=font_axis)\n",
    "        plt.xlabel('\\nYears\\n', fontdict=font_axis)\n",
    "        plt.legend()\n",
    "    \n",
    "        path = '/home/dbda/Desktop/'\n",
    "        name = 'Trends of ' +crime\n",
    "        plt.savefig(path+name+'.jpg')\n",
    "        plt.show()\n",
    "    \n",
    "    #print(df.head)\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,9))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(df_total_num.YEAR, df_total_num[col_name], '--', linewidth=2, label='Trend of All Incidents',color=\"r\")\n",
    "    text ='All Incidents \\n'\n",
    "    plt.title(text, fontdict=font_title)\n",
    "    plt.ylabel('Total Number of Incidents\\n', fontdict=font_axis)\n",
    "    plt.legend()\n",
    "    \n",
    "    path = '/home/dbda/Desktop/'\n",
    "    name = 'Trends all incidents'\n",
    "    plt.savefig(path+name+'.jpg')\n",
    "    plt.show()\n",
    "    return df_total_num   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_trends(df,group_by_col=['YEAR'], category='TOTAL (MANUAL)', col_name='number_incidents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pylab as plt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "subset_frame =  df[['YEAR','PLACE_OCCURENCE','DACOITY_NUM', 'ROBBERY_NO', 'BURGLARY_NO','THEFT_NO']]\n",
    "df_total_num=subset_frame[subset_frame['PLACE_OCCURENCE']=='TOTAL (MANUAL)'].groupby('YEAR')[['DACOITY_NUM', 'ROBBERY_NO', 'BURGLARY_NO','THEFT_NO']].agg('sum').reset_index()\n",
    "    #print(df_none.head)\n",
    "    \n",
    "df_total_num['number_incidents']= df_total_num.iloc[:, 1:5].sum(axis=1)\n",
    "df_arima=df_total_num[['YEAR','number_incidents']]\n",
    "\n",
    "\n",
    "\n",
    "#df_arima.columns = ['','']\n",
    "#df_arima['YEAR'] = pd.to_datetime(df_arima['YEAR'],format=\"%Y\")\n",
    "#df_arima.set_index('YEAR')\n",
    "\n",
    "#df_arima['YEAR'] = pd.to_datetime(df_arima['YEAR'],format=\"%Y\")\n",
    "\n",
    "\n",
    "#df_arima.to_csv('arimadata.csv', sep=',',index=0)\n",
    "\n",
    "\n",
    "df_arima = pd.read_csv('arimadata.csv')#, parse_dates=[0], index_col=0, squeeze=True, date_parser=dateparse)\n",
    "df_arima['YEAR']=df_arima.YEAR.astype(str)\n",
    "df_arima['number_incidents']=df_arima.number_incidents.astype(float)\n",
    "#print(df_arima)\n",
    "#print(type(df_arima))\n",
    "\n",
    "dateparse = lambda dates: pd.datetime.strptime(dates, '%Y')\n",
    "df_arima = pd.read_csv('arimadata.csv',parse_dates=[0],date_parser=dateparse)\n",
    "\n",
    "df_correlation=df_arima\n",
    "\n",
    "\n",
    "\n",
    "model = ARIMA(df_arima['number_incidents'].astype(float),order=(4,1,0), dates=df_arima['YEAR'])\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())\n",
    "# plot residual errors\n",
    "residuals = DataFrame(model_fit.resid)\n",
    "#Line plot of the residual errors, suggesting that there may still be some trend information not captured by the model.\n",
    "residuals.plot()\n",
    "pyplot.show()\n",
    "#Density plot of the residual error values, suggesting the errors are Gaussian, but may not be centered on zero.\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "print(residuals.describe())\n",
    "\n",
    "predictions_ARIMA_diff = pd.Series(model_fit.fittedvalues, copy=True)\n",
    "print(\"##############################################################\")\n",
    "print (predictions_ARIMA_diff.head())\n",
    "\n",
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "print(\"##############################################################\")\n",
    "print (predictions_ARIMA_diff_cumsum.head())\n",
    "\n",
    "ts = df_correlation['number_incidents']\n",
    "\n",
    "predictions_ARIMA_log = pd.Series(df_arima.ix[0], index=df_arima.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_ARIMA_log.head()\n",
    "\n",
    "print(\"Actual Value\")\n",
    "\n",
    "print(ts)\n",
    "\n",
    "print(\"predicted value\")\n",
    "print(predictions_ARIMA_log)\n",
    "\n",
    "print(df_arima)\n",
    "\n",
    "predictions_ARIMA = np.exp(predictions_ARIMA_log.astype(float))\n",
    "plt.plot(ts)\n",
    "plt.plot(predictions_ARIMA)\n",
    "plt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA_log+ts)**2)/len(ts)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autocorrelation for a large number of lags in the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arima = pd.read_csv('arimadata.csv')#, parse_dates=[0], index_col=0, squeeze=True, date_parser=dateparse)\n",
    "df_arima['YEAR']=df_arima.YEAR.astype(int)\n",
    "df_arima['number_incidents']=df_arima.number_incidents.astype(float)\n",
    "\n",
    "dateparse = lambda dates: pd.datetime.strptime(dates, '%Y')\n",
    "df_arima = pd.read_csv('arimadata.csv',parse_dates=[0],header=0, index_col=0, squeeze=True, date_parser=dateparse)\n",
    "autocorrelation_plot(df_arima)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Check Stationarity of a Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_correlation.index)\n",
    "\n",
    "ts = df_correlation['number_incidents']\n",
    "from datetime import datetime\n",
    "ts[0:13]\n",
    "\n",
    "print(ts.dtypes)\n",
    "plt.plot(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Stationariety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(timeseries):\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = pd.rolling_mean(timeseries, window=12)\n",
    "    rolstd = pd.rolling_std(timeseries, window=12)\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_correlation.index)\n",
    "\n",
    "ts = df_correlation['number_incidents']\n",
    "print(ts)\n",
    "from datetime import datetime\n",
    "ts[0:13]\n",
    "\n",
    "print(ts.dtypes)\n",
    "test_stationarity(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating and eliminating trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log = np.log(ts)\n",
    "plt.plot(ts_log)\n",
    "\n",
    "##In this simpler case, it is easy to see a forward trend in the data. \n",
    "#But its not very intuitive in presence of noise. So we can use some techniques to estimate or model this trend and then remove it from the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation – taking average for a time period like monthly/weekly averages,Smoothing – taking rolling averages,Polynomial Fitting – fit a regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg = pd.rolling_mean(ts_log,2)\n",
    "plt.plot(ts_log)\n",
    "plt.plot(moving_avg, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_moving_avg_diff = ts_log - moving_avg\n",
    "ts_log_moving_avg_diff.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_moving_avg_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_moving_avg_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expwighted_avg = pd.ewma(ts_log, halflife=2)\n",
    "plt.plot(ts_log)\n",
    "plt.plot(expwighted_avg, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_ewma_diff = ts_log - expwighted_avg\n",
    "test_stationarity(ts_log_ewma_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(df_correlation, order=(2, 1, 0))  \n",
    "results_AR = model.fit(disp=-1)  \n",
    "plt.plot(ts_log_diff)\n",
    "plt.plot(results_AR.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_diff)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_linear=df_total_num.drop('number_incidents',axis=1)\n",
    "df_linear.columns\n",
    "print(df_linear)\n",
    "\n",
    "sns.pairplot(df_linear,x_vars=['DACOITY_NUM','ROBBERY_NO','BURGLARY_NO'],y_vars=['YEAR'],kind='reg',size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols=df_linear[['DACOITY_NUM','ROBBERY_NO','BURGLARY_NO','THEFT_NO']]\n",
    "\n",
    "X=feature_cols\n",
    "\n",
    "X\n",
    "\n",
    "y=df_linear['YEAR']\n",
    "\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model using test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linearReg = LinearRegression()\n",
    "\n",
    "linearReg.fit(X_train,y_train)\n",
    "\n",
    "print(linearReg.intercept_)\n",
    "print(linearReg.coef_)\n",
    "\n",
    "print(df_linear.columns,linearReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
